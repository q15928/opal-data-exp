{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import to_timestamp, unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Opal tap\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"5\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data/time_loc_20161226-20170101.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_csv_file = (spark.read.format(\"csv\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .option(\"inferSchema\", \"true\")\n",
    "#                 .option(\"mode\", \"FAILFAST\")\n",
    "                .load(\"./data/time_loc_20161226-20170101.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169554"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_csv_file.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mode: string (nullable = true)\n",
      " |-- date: integer (nullable = true)\n",
      " |-- tap: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- loc: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_csv_file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---+-----+----+-----+\n",
      "|mode|    date|tap| time| loc|count|\n",
      "+----+--------+---+-----+----+-----+\n",
      "| bus|20161226|off|00:00|2000|   21|\n",
      "| bus|20161226|off|00:00|2010|   20|\n",
      "| bus|20161226|off|00:00|2021|   18|\n",
      "| bus|20161226|off|00:00|2022|   50|\n",
      "| bus|20161226|off|00:00|2031|   22|\n",
      "| bus|20161226|off|00:00|2033|   22|\n",
      "| bus|20161226|off|00:00|2050|   23|\n",
      "| bus|20161226|off|00:00|2155|   21|\n",
      "| bus|20161226|off|00:15|2000|   37|\n",
      "| bus|20161226|off|00:15|2026|   31|\n",
      "+----+--------+---+-----+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_csv_file.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     mode|\n",
      "+---------+\n",
      "|    ferry|\n",
      "|lightrail|\n",
      "|    train|\n",
      "|      bus|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_csv_file.select(\"mode\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data is messy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### missing the \"tap\" field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----+--------------------+---+-----+\n",
      "| mode|    date|  tap|                time|loc|count|\n",
      "+-----+--------+-----+--------------------+---+-----+\n",
      "|ferry|20170101|00:00|Circular Quay, No...| 61| null|\n",
      "|ferry|20170101|00:00|         Manly Wharf|148| null|\n",
      "|ferry|20170101|00:15|Circular Quay, No...|281| null|\n",
      "|ferry|20170101|00:15|  Balmain East Wharf| 42| null|\n",
      "|ferry|20170101|00:15|         Manly Wharf| 62| null|\n",
      "|ferry|20170101|00:30|Circular Quay, No...|365| null|\n",
      "|ferry|20170101|00:30|Circular Quay, No...| 77| null|\n",
      "|ferry|20170101|00:30|  Balmain East Wharf| 44| null|\n",
      "|ferry|20170101|00:30|    Kirribilli Wharf| 20| null|\n",
      "|ferry|20170101|00:30|         Manly Wharf| 40| null|\n",
      "+-----+--------+-----+--------------------+---+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_csv_file.filter(\"mode = 'ferry' AND date = '20170101'\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### time is incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---+----+---+-----+\n",
      "|mode|    date|tap|time|loc|count|\n",
      "+----+--------+---+----+---+-----+\n",
      "| bus|20161226|off|  -1| -1|10594|\n",
      "| bus|20161226| on|  -1| -1|  246|\n",
      "| bus|20161227|off|  -1| -1|11203|\n",
      "| bus|20161227| on|  -1| -1|  105|\n",
      "| bus|20161228|off|  -1| -1|15858|\n",
      "| bus|20161228| on|  -1| -1|  149|\n",
      "| bus|20161229|off|  -1| -1|16212|\n",
      "| bus|20161229| on|  -1| -1|  165|\n",
      "| bus|20161230|off|  -1| -1|15957|\n",
      "| bus|20161230| on|  -1| -1|  195|\n",
      "+----+--------+---+----+---+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_csv_file.filter(~raw_csv_file[\"time\"].rlike(\"(\\d\\d:\\d\\d)\")).show(10)\n",
    "# raw_csv_file.filter(\"time rlike '\\d\\d:\\d\\d'\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### location is unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---+-----+---+-----+\n",
      "|mode|    date|tap| time|loc|count|\n",
      "+----+--------+---+-----+---+-----+\n",
      "| bus|20161226|off|07:30| -1|   28|\n",
      "| bus|20161226|off|07:45| -1|   57|\n",
      "| bus|20161226|off|08:15| -1|   36|\n",
      "| bus|20161226|off|08:30| -1|   41|\n",
      "| bus|20161226|off|08:45| -1|   36|\n",
      "| bus|20161226|off|09:00| -1|   53|\n",
      "| bus|20161226|off|09:30| -1|   77|\n",
      "| bus|20161226|off|09:45| -1|   65|\n",
      "| bus|20161226|off|   -1| -1|10594|\n",
      "| bus|20161226|off|10:00| -1|   62|\n",
      "+----+--------+---+-----+---+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----+--------+---+----+-------+-----+\n",
      "| mode|    date|tap|time|    loc|count|\n",
      "+-----+--------+---+----+-------+-----+\n",
      "|ferry|20161226|off|  -1|UNKNOWN| 1479|\n",
      "|ferry|20161226| on|  -1|UNKNOWN| 2070|\n",
      "|ferry|20161227|off|  -1|UNKNOWN| 1667|\n",
      "|ferry|20161227| on|  -1|UNKNOWN| 2131|\n",
      "|ferry|20161228|off|  -1|UNKNOWN| 1722|\n",
      "|ferry|20161228| on|  -1|UNKNOWN| 2216|\n",
      "|ferry|20161229|off|  -1|UNKNOWN| 1497|\n",
      "|ferry|20161229| on|  -1|UNKNOWN| 1996|\n",
      "|ferry|20161230|off|  -1|UNKNOWN| 2028|\n",
      "|ferry|20161230| on|  -1|UNKNOWN| 2181|\n",
      "+-----+--------+---+----+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_csv_file.filter(\"loc = '-1'\").show(10)\n",
    "raw_csv_file.filter(\"loc = 'UNKNOWN'\").show(10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode,date,tap,time,loc,count\n",
    "opal_schema = StructType([\n",
    "    StructField(\"mode\", StringType(), True),\n",
    "    StructField(\"timestamp_str\", StringType(), True),\n",
    "    StructField(\"tap\", StringType(), True),\n",
    "    StructField(\"loc\", StringType(), True),\n",
    "    StructField(\"count\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_record(line):\n",
    "    \"\"\"\n",
    "    Validate the records to have the correct time, and loc fields\n",
    "    \n",
    "    Valid formats are:\n",
    "    'lightrail,20170101,00:15,Convention Centre Light Rail,48'\n",
    "    'ferry,20170101,00:00,\"Circular Quay, No. 3 Wharf\",61'\n",
    "    'ferry,20161231,on,18:15,Watsons Bay Wharf,18'\n",
    "    'ferry,20161231,on,18:15,\"Circular Quay, No. 3 Wharf\",68'\n",
    "\n",
    "    \"\"\"\n",
    "    if \"-1\" in line or \"unknown\" in line.lower() or \"mode\" in line.lower():\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_record(line):\n",
    "    \"\"\"\n",
    "    Parse and clean the records with following steps:\n",
    "    1. Parse the line and put them aligning with the header \n",
    "       [mode,date,tap,time,loc,count]. Some records are missing\n",
    "       the top on/off information.\n",
    "    2. Add timestamp field by combining date and time.\n",
    "    3. Drop date and time fields.\n",
    "    \n",
    "    \"\"\"\n",
    "    res = None\n",
    "    p = r'(\\w+),(\\d{8}),(\\w+),(\\d\\d:\\d\\d),\"(.+)\",(\\d+)'\n",
    "    m = re.match(p, line)\n",
    "    if m:\n",
    "        res = list(m.groups())\n",
    "    else:\n",
    "        p = r'(\\w+),(\\d{8}),(\\w+),(\\d\\d:\\d\\d),(.+),(\\d+)'\n",
    "        m = re.match(p, line)\n",
    "        if m:\n",
    "            res = list(m.groups())\n",
    "        else:\n",
    "            p = r'(\\w+),(\\d{8}),(\\d\\d:\\d\\d),\"(.+)\",(\\d+)'\n",
    "            m = re.match(p, line)\n",
    "            if m:\n",
    "                res = list(m.groups())\n",
    "            else:\n",
    "                p = r'(\\w+),(\\d{8}),(\\d\\d:\\d\\d),(.+),(\\d+)'\n",
    "                m = re.match(p, line)\n",
    "                if m:\n",
    "                    res = list(m.groups())\n",
    "    if res is None:\n",
    "        print(line)\n",
    "        return\n",
    "    \n",
    "    if len(res) == 5:\n",
    "        res.insert(2, \"\")\n",
    "        \n",
    "    return [res[0], res[1]+' '+res[3], res[2], res[4], int(res[5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        if validate_record(line):\n",
    "            parse_record(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169052"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_file = (spark.read.text(file_path)\n",
    "    .rdd.map(lambda r: r[0])\n",
    "    .filter(validate_rec)\n",
    "    .map(parse_record))\n",
    "opal_df = raw_file.toDF(schema=opal_schema)\n",
    "opal_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+---+----+-----+\n",
      "|mode| timestamp_str|tap| loc|count|\n",
      "+----+--------------+---+----+-----+\n",
      "| bus|20161226 00:00|off|2000|   21|\n",
      "| bus|20161226 00:00|off|2010|   20|\n",
      "| bus|20161226 00:00|off|2021|   18|\n",
      "| bus|20161226 00:00|off|2022|   50|\n",
      "| bus|20161226 00:00|off|2031|   22|\n",
      "| bus|20161226 00:00|off|2033|   22|\n",
      "| bus|20161226 00:00|off|2050|   23|\n",
      "| bus|20161226 00:00|off|2155|   21|\n",
      "| bus|20161226 00:15|off|2000|   37|\n",
      "| bus|20161226 00:15|off|2026|   31|\n",
      "+----+--------------+---+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opal_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = opal_df.withColumn(\"timestamp\", to_timestamp(\"timestamp_str\", \"yyyyMMdd HH:mm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+---+--------------------+-----+-------------------+\n",
      "| mode| timestamp_str|tap|                 loc|count|          timestamp|\n",
      "+-----+--------------+---+--------------------+-----+-------------------+\n",
      "|train|20161226 00:00|off|    Ashfield Station|   75|2016-12-26 00:00:00|\n",
      "|train|20161226 00:00|off|   Bankstown Station|   23|2016-12-26 00:00:00|\n",
      "|train|20161226 00:00|off|Bondi Junction St...|   46|2016-12-26 00:00:00|\n",
      "|train|20161226 00:00|off|     Burwood Station|   20|2016-12-26 00:00:00|\n",
      "|train|20161226 00:00|off|     Campsie Station|   63|2016-12-26 00:00:00|\n",
      "|train|20161226 00:00|off|     Central Station|  102|2016-12-26 00:00:00|\n",
      "|train|20161226 00:00|off|    Eastwood Station|   23|2016-12-26 00:00:00|\n",
      "|train|20161226 00:00|off|      Epping Station|   39|2016-12-26 00:00:00|\n",
      "|train|20161226 00:00|off|   Fairfield Station|   34|2016-12-26 00:00:00|\n",
      "|train|20161226 00:00|off|     Hornsby Station|   19|2016-12-26 00:00:00|\n",
      "+-----+--------------+---+--------------------+-----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.filter(\"timestamp_str = '20161226 00:00' and mode = 'train'\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView(\"opal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-------------------+-----+\n",
      "| mode|          dt_hourly|                loc|total|\n",
      "+-----+-------------------+-------------------+-----+\n",
      "|train|2016-12-28 17:00:00|Albion Park Station|   20|\n",
      "|train|2017-01-01 02:00:00|Albion Park Station|   19|\n",
      "|train|2016-12-26 05:00:00|    Allawah Station|   24|\n",
      "|train|2016-12-26 06:00:00|    Allawah Station|   49|\n",
      "|train|2016-12-26 07:00:00|    Allawah Station|   40|\n",
      "|train|2016-12-26 08:00:00|    Allawah Station|   97|\n",
      "|train|2016-12-26 09:00:00|    Allawah Station|  153|\n",
      "|train|2016-12-26 10:00:00|    Allawah Station|  153|\n",
      "|train|2016-12-26 11:00:00|    Allawah Station|  145|\n",
      "|train|2016-12-26 12:00:00|    Allawah Station|  121|\n",
      "|train|2016-12-26 13:00:00|    Allawah Station|  157|\n",
      "|train|2016-12-26 14:00:00|    Allawah Station|  108|\n",
      "|train|2016-12-26 15:00:00|    Allawah Station|  111|\n",
      "|train|2016-12-26 16:00:00|    Allawah Station|  126|\n",
      "|train|2016-12-26 17:00:00|    Allawah Station|  123|\n",
      "|train|2016-12-26 18:00:00|    Allawah Station|  131|\n",
      "|train|2016-12-26 19:00:00|    Allawah Station|  120|\n",
      "|train|2016-12-26 20:00:00|    Allawah Station|   90|\n",
      "|train|2016-12-26 21:00:00|    Allawah Station|   85|\n",
      "|train|2016-12-26 22:00:00|    Allawah Station|   54|\n",
      "|train|2016-12-26 23:00:00|    Allawah Station|   43|\n",
      "|train|2016-12-27 05:00:00|    Allawah Station|   23|\n",
      "|train|2016-12-27 06:00:00|    Allawah Station|   21|\n",
      "|train|2016-12-27 07:00:00|    Allawah Station|   40|\n",
      "+-----+-------------------+-------------------+-----+\n",
      "only showing top 24 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_stmt = \"\"\"\n",
    "    SELECT \n",
    "        mode, \n",
    "        CAST(concat(DATE(timestamp), ' ', lpad(HOUR(timestamp), 2, '0'), ':00') AS timestamp) AS dt_hourly,\n",
    "        loc, sum(count) AS total\n",
    "    FROM opal\n",
    "    WHERE mode = 'train' -- AND loc = 'Central Station'\n",
    "    GROUP BY 1, 2, 3\n",
    "    ORDER BY 3, 2\n",
    "\"\"\"\n",
    "\n",
    "df3 = spark.sql(sql_stmt)\n",
    "df3.show(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19550"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file.saveAsTextFile(\"/tmp/opal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opal_df.select(\"timestamp_str\").distinct().count()\n",
    "opal_df.write.format(\"csv\").mode(\"overwrite\").save(\"/tmp/opal.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
